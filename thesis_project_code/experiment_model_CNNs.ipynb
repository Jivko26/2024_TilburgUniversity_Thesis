{"cells":[{"cell_type":"markdown","metadata":{"id":"amMZWTSenDs5"},"source":["# 1. Initial Steps and Data Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEnaHANQAp3p"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715000176546,"user":{"displayName":"Jivko Parapanov","userId":"13549335107747025520"},"user_tz":-120},"id":"5XnG1MZ_As85","outputId":"b4a4ed4d-7869-43b4-f386-44c4a897113d"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","SEED = 42\n","\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnUUpJqRAwPj"},"outputs":[],"source":["DATA_DIRECTORY = './Data/hyperaktiv_with_controls/hyperaktiv_with_controls/'\n","VALID_IDs = [1, 3, 5, 11, 15, 19, 20, 21, 22, 23, 24, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 68, 71, 73, 75, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 101, 104, 105]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":3923,"status":"ok","timestamp":1715000180463,"user":{"displayName":"Jivko Parapanov","userId":"13549335107747025520"},"user_tz":-120},"id":"j5CUst8WAzKJ","outputId":"6070eef6-cfba-490b-f0ee-bf283e538f87"},"outputs":[],"source":["demographic_data = pd.read_csv(f'{DATA_DIRECTORY}patient_info.csv', sep=';')\n","#plot the balance of the ADHD class in the demographic_data for every record that has ID in VALID_IDs. Insert labels and make it more appealing\n","demographic_data = demographic_data[demographic_data['ID'].isin(VALID_IDs)]\n","# Extract labels for these IDs\n","labels = demographic_data['ADHD'].values\n","\n","# Output the labels to verify\n","print(labels)\n","\n","demographic_data['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1715000180464,"user":{"displayName":"Jivko Parapanov","userId":"13549335107747025520"},"user_tz":-120},"id":"kj_rZbieA0qe","outputId":"872bfe9c-2675-4d41-f86c-08439d683d80"},"outputs":[],"source":["# I want to noe the IDS of the control and ADHD patients\n","control_ids = demographic_data[demographic_data['ADHD'] == 0]['ID'].values\n","adhd_ids = demographic_data[demographic_data['ADHD'] == 1]['ID'].values\n","\n","print(f'Number of control patients: {len(control_ids)}; IDS: {control_ids}')\n","print(f'Number of ADHD patients: {len(adhd_ids)}; IDS: {adhd_ids}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","\n","def enhanced_split_dataset(ids, labels, train_ratio=0.80, val_ratio=0.20, random_seed=42):\n","    # Convert ratios to a useable format for StratifiedShuffleSplit\n","    splits = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=random_seed)\n","\n","    # Split the data into train and validation sets\n","    train_ids, val_ids = next(splits.split(ids, labels))\n","\n","    # Convert indices to actual IDs\n","    train_ids = ids[train_ids]\n","    val_ids = ids[val_ids]\n","\n","    return train_ids, val_ids\n","\n","# Example usage:\n","# Assume labels is an array of labels corresponding to VALID_IDs in the same order\n","train_ids, val_ids = enhanced_split_dataset(np.array(VALID_IDs), np.array(labels))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["demographic_data_train = demographic_data[demographic_data['ID'].isin(train_ids)]\n","demographic_data_train['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the train dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()\n","\n","\n","demographic_data_val = demographic_data[demographic_data['ID'].isin(val_ids)]\n","demographic_data_val['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the valdiation dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def scale_data(data):\n","    scaler = RobustScaler()\n","    return scaler.fit_transform(data.reshape(-1, 1)).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_data(sample, demographic_data):\n","    patients_data = {}  # Dictionary to store data\n","\n","    for patient_id in sample:\n","        hrv_data = pd.read_csv(f'{DATA_DIRECTORY}hrv_data_train/patient_hr_{patient_id}.csv', sep=';')\n","        activity_data = pd.read_csv(f'{DATA_DIRECTORY}activity_data_train/patient_activity_{patient_id}.csv', sep=';')\n","        labels =  demographic_data[demographic_data['ID'] == patient_id]['ADHD'].values[0]  # Get the ADHD label for the patient\n","\n","    # Convert TIMESTAMP to datetime\n","        hrv_data['TIMESTAMP'] = pd.to_datetime(hrv_data['TIMESTAMP'], errors='coerce')\n","        activity_data['TIMESTAMP'] = pd.to_datetime(activity_data['TIMESTAMP'], errors='coerce')\n","\n","    # Setting TIMESTAMP as index and checking for NaNs in data columns\n","        df_hrv = pd.DataFrame(data=hrv_data).set_index('TIMESTAMP')\n","        df_activity = pd.DataFrame(data=activity_data).set_index('TIMESTAMP')\n","\n","    # Now resample for a 1 second interval\n","        df_hrv = df_hrv.resample('1S').mean()\n","        df_activity = df_activity.resample('1S').mean()\n","\n","    # Fill NaNs in HRV and Activity before resampling\n","        df_hrv['HRV'] = df_hrv['HRV'].fillna(method='ffill')  # Forward fill as an example\n","        df_activity['ACTIVITY'] = df_activity['ACTIVITY'].fillna(method='ffill')  # Forward fill as an example\n","\n","    # Scale data\n","        hrv_series = scale_data(df_hrv['HRV'].values)\n","        activity_series = scale_data(activity_data['ACTIVITY'].values)\n","\n","    # Store in dictionary\n","        patients_data[patient_id] = {\n","        'hrv': hrv_series,\n","        'activity': activity_series,\n","        'adhd': labels\n","        }\n","\n","    return patients_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","    \n","\n","# def truncate_and_pad_sequences(sequence, max_length):\n","#     # Truncate if necessary and pad sequences that are too short\n","#     sequence = sequence[:max_length]  # Truncate if longer than max_length\n","#     if len(sequence) < max_length:\n","#         sequence = np.pad(sequence, (0, max_length - len(sequence)), 'constant', constant_values=0)  # Pad with zeros\n","#     return sequence\n","\n","\n","# def load_data(sample, demographic_data):\n","#     patients_data = {}\n","\n","#     for patient_id in sample:\n","#         hrv_data = pd.read_csv(f'{DATA_DIRECTORY}hrv_data_train/patient_hr_{patient_id}.csv', sep=';')\n","#         activity_data = pd.read_csv(f'{DATA_DIRECTORY}activity_data_train/patient_activity_{patient_id}.csv', sep=';')\n","#         label = demographic_data[demographic_data['ID'] == patient_id]['ADHD'].values[0]\n","\n","#         # Scale data\n","#         hrv_series = scale_data(hrv_data['HRV'].values)\n","#         activity_series = scale_data(activity_data['ACTIVITY'].values)\n","\n","#         # Store in dictionary\n","#         patients_data[patient_id] = {\n","#             'hrv': hrv_series,\n","#             'activity': activity_series,\n","#             'adhd': label\n","#         }\n","\n","#     return patients_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sample usage with train_ids and demographic_data_train previously defined\n","train_data = load_data(train_ids, demographic_data=demographic_data_train)\n","val_data = load_data(val_ids, demographic_data=demographic_data_val)\n","\n","all_data = {\n","    'train': train_data,\n","    'val': val_data,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# print a single sample to check data\n","train_sample_pre_trunc = all_data['train'][train_ids[0]]\n","print(train_sample_pre_trunc['hrv'])\n","print(train_sample_pre_trunc['activity'])\n","print(train_sample_pre_trunc['adhd'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# print a single sample to check data\n","val_sample_pre_trunc = all_data['val'][val_ids[0]]\n","print(val_sample_pre_trunc['hrv'])\n","print(val_sample_pre_trunc['activity'])\n","print(val_sample_pre_trunc['adhd'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example of calculating a percentile-based max_length\n","lengths = [len(data['hrv']) for patient_id, data in all_data['train'].items()]\n","max_length_train = int(np.percentile(lengths, 95))  # Using 95th percentile\n","\n","max_length_train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_lengths = [len(data['hrv']) for patient_id, data in all_data['val'].items()]\n","max_length_val = int(np.percentile(val_lengths, 95))  # Using 95th percentile\n","\n","max_length_val"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def truncate_and_pad_sequences(sequence, max_length):\n","    # Truncate if necessary and pad sequences that are too short\n","    sequence = sequence[:max_length]  # Truncate if longer than max_length\n","    if len(sequence) < max_length:\n","        sequence = np.pad(sequence, (0, max_length - len(sequence)), 'constant', constant_values=0)  # Pad with zeros\n","    return sequence\n","\n","\n","# Use the maximum value from both datasets\n","GLOBAL_MAX_LENGHT = max(max_length_train, max_length_val)\n","# Truncate and pad sequences for all patients in the dataset\n","for patient_id, data in all_data['train'].items():\n","    hrv_data = data['hrv']\n","    activity_data = data['activity']\n","\n","    all_data['train'][patient_id]['hrv'] = truncate_and_pad_sequences(hrv_data, GLOBAL_MAX_LENGHT)\n","    all_data['train'][patient_id]['activity'] = truncate_and_pad_sequences(activity_data, GLOBAL_MAX_LENGHT)\n","\n","for patient_id, data in all_data['val'].items():\n","    hrv_data = data['hrv']\n","    activity_data = data['activity']\n","\n","    all_data['val'][patient_id]['hrv'] = truncate_and_pad_sequences(hrv_data, GLOBAL_MAX_LENGHT)\n","    all_data['val'][patient_id]['activity'] = truncate_and_pad_sequences(activity_data, GLOBAL_MAX_LENGHT)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print a single sample to check data\n","train_sample_post_trunc = all_data['train'][train_ids[0]]\n","print(train_sample_post_trunc['hrv'])\n","print(train_sample_post_trunc['activity'])\n","print(train_sample_post_trunc['adhd'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print a single sample to check data\n","val_sample_post_trunc = all_data['val'][val_ids[0]]\n","print(val_sample_post_trunc['hrv'])\n","print(val_sample_post_trunc['activity'])\n","print(val_sample_post_trunc['adhd'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DualBranch1DCNN(nn.Module):\n","    def __init__(self, use_dropout=False, dropout_rate=0.5, use_batch_norm=False):\n","        super(DualBranch1DCNN, self).__init__()\n","        # Parameters for the conv layers (set statically here)\n","        kernel_size = 5\n","        stride = 2\n","        pool_size = 2\n","        self.hrv_branch = self.build_branch(use_dropout, dropout_rate, use_batch_norm, kernel_size, stride, pool_size)\n","        self.activity_branch = self.build_branch(use_dropout, dropout_rate, use_batch_norm, kernel_size, stride, pool_size)\n","\n","        # Final layers\n","        self.final_layers = nn.Sequential(\n","            nn.Linear(256, 128),  # Adjusted assuming a COMMON_INPUT_LENGTH\n","            nn.ReLU()\n","        )\n","        if use_batch_norm:\n","            self.final_layers.add_module('final_batch_norm', nn.BatchNorm1d(128))\n","        if use_dropout:\n","            self.final_layers.add_module('final_dropout', nn.Dropout(dropout_rate))\n","        self.final_layers.add_module('final_output', nn.Linear(128, 1))  # Binary classification\n","\n","    def build_branch(self, use_dropout, dropout_rate, use_batch_norm, kernel_size, stride, pool_size):\n","        layers = [\n","            nn.Conv1d(1, 16, kernel_size, stride=stride),\n","            nn.ReLU(),\n","            nn.MaxPool1d(pool_size),\n","            nn.Conv1d(16, 32, kernel_size, stride=stride),\n","            nn.ReLU(),\n","            nn.MaxPool1d(pool_size),\n","            nn.Flatten(),\n","            nn.Linear(32 * self.calculate_output_length(GLOBAL_MAX_LENGHT, kernel_size, stride, pool_size), 128),\n","            nn.ReLU()\n","        ]\n","        if use_batch_norm:\n","            layers.append(nn.BatchNorm1d(128))\n","        if use_dropout:\n","            layers.append(nn.Dropout(dropout_rate))\n","        return nn.Sequential(*layers)\n","\n","    def calculate_output_length(self, input_length, kernel_size, stride, pool_size):\n","        output_length = input_length\n","        for _ in range(2):  # Two layers of conv+pool\n","            output_length = (output_length - (kernel_size - 1) - 1) // stride + 1\n","            output_length = (output_length - (pool_size - 1) - 1) // pool_size + 1\n","        return output_length\n","\n","    def forward(self, hrv_data, activity_data):\n","        hrv_features = self.hrv_branch(hrv_data)\n","        activity_features = self.activity_branch(activity_data)\n","        combined_features = torch.cat((hrv_features, activity_features), dim=1)\n","        output = self.final_layers(combined_features)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ADHDData(Dataset):\n","    def __init__(self, data):\n","        self.hrv_data = [torch.tensor(data[pid]['hrv'], dtype=torch.float32).unsqueeze(0) for pid in data]\n","        self.activity_data = [torch.tensor(data[pid]['activity'], dtype=torch.float32).unsqueeze(0) for pid in data]\n","        self.labels = [data[pid]['adhd'] for pid in data]\n","    \n","    def __len__(self):\n","        return len(self.labels)\n","    \n","    def __getitem__(self, index):\n","        return self.hrv_data[index], self.activity_data[index], self.labels[index]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert data dictionaries into datasets\n","train_dataset = ADHDData(all_data['train'])\n","val_dataset = ADHDData(all_data['val'])\n","\n","print(f'Train dataset size: {len(train_dataset)}')\n","print(f'Validation dataset size: {len(val_dataset)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(model, train_loader, val_loader, device, config):\n","    criterion = torch.nn.BCEWithLogitsLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate']) if config['optimizer_name'] == 'adam' \\\n","                else torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config.get('momentum', 0.9))\n","    \n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    best_val_accuracy = 0\n","    best_metrics = {}\n","\n","    for epoch in range(config['num_epochs']):\n","        model.train()\n","        total = 0\n","        correct = 0\n","        train_loss = 0\n","        \n","        for hrv_data, activity_data, labels in train_loader:\n","            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device).float()\n","            \n","            optimizer.zero_grad()\n","            outputs = model(hrv_data, activity_data)\n","            loss = criterion(outputs, labels.unsqueeze(1))\n","            loss.backward()\n","            optimizer.step()\n","            \n","            train_loss += loss.item()\n","            predicted = torch.sigmoid(outputs).round()\n","            total += labels.size(0)\n","            correct += (predicted == labels.unsqueeze(1)).sum().item()\n","\n","        train_accuracy = 100 * correct / total\n","        train_losses.append(train_loss / len(train_loader))\n","        train_accuracies.append(train_accuracy)\n","\n","        # Validation\n","        val_loss, val_accuracy = validate_model(model, val_loader, criterion, device)\n","        val_losses.append(val_loss)\n","        val_accuracies.append(val_accuracy)\n","\n","        print(f'Epoch {epoch+1}/{config[\"num_epochs\"]}:')\n","        print(f'  Training Loss: {train_loss / len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%')\n","        print(f'  Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n","\n","        # Save best model based on validation accuracy\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            best_metrics = {\n","                'train_loss': train_losses,\n","                'val_loss': val_losses,\n","                'train_accuracy': train_accuracies,\n","                'val_accuracy': val_accuracies\n","            }\n","            print(\" Best model updated.\")\n","\n","    return best_metrics\n","\n","\n","def validate_model(model, val_loader, criterion, device):\n","    model.eval()\n","    val_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for hrv_data, activity_data, labels in val_loader:\n","            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device).float()\n","            outputs = model(hrv_data, activity_data)\n","            loss = criterion(outputs, labels.unsqueeze(1))\n","            val_loss += loss.item()\n","            predicted = torch.sigmoid(outputs).round()\n","            correct += (predicted == labels.unsqueeze(1)).sum().item()\n","            total += labels.size(0)\n","    \n","    accuracy = 100 * correct / total\n","    return val_loss / len(val_loader), accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# configurations = [\n","#     {'config_id': 1, 'optimizer_name': 'adam', 'batch_size': 64, 'learning_rate': 0.001, 'num_epochs': 100, 'use_dropout': True, 'dropout_rate': 0.5, 'use_batch_norm': True},\n","#     {'config_id': 2, 'optimizer_name': 'adam', 'batch_size': 32, 'learning_rate': 0.01, 'num_epochs': 80, 'use_dropout': True, 'dropout_rate': 0.3, 'use_batch_norm': True},\n","#     {'config_id': 3, 'optimizer_name': 'adam', 'batch_size': 32, 'learning_rate': 0.01, 'num_epochs': 50, 'use_dropout': True, 'dropout_rate': 0.1, 'use_batch_norm': True},\n","#     {'config_id': 4, 'optimizer_name': 'sgd', 'batch_size': 32, 'learning_rate': 0.01, 'num_epochs': 30, 'use_dropout': True, 'dropout_rate': 0.1, 'use_batch_norm': True, 'momentum': 0.9},\n","#     {'config_id': 5, 'optimizer_name': 'adam', 'batch_size': 128, 'learning_rate': 0.05, 'num_epochs': 20, 'use_dropout': True, 'dropout_rate': 0.2, 'use_batch_norm': True},\n","#     {'config_id': 6, 'optimizer_name': 'sgd', 'batch_size': 16, 'learning_rate': 0.001, 'num_epochs': 40, 'use_dropout': True, 'dropout_rate': 0.5, 'use_batch_norm': True, 'momentum': 0.8}\n","# ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","import itertools\n","\n","def generate_configurations():\n","    # Define your hyperparameter space\n","    optimizer_names = ['adam', 'sgd']\n","    batch_sizes = [16, 32, 64, 128]\n","    learning_rates = [0.001, 0.01, 0.05]\n","    num_epochs = [20, 30, 40, 50, 60, 70]\n","    use_dropout = [True, False]\n","    dropout_rates = [0.1, 0.2, 0.3, 0.5]\n","    use_batch_norm = [True, False]\n","    momenta = [0.8, 0.9]  # Only for 'sgd'\n","\n","    # Cartesian product to generate all combinations\n","    all_combinations = list(itertools.product(optimizer_names, batch_sizes, learning_rates, num_epochs,\n","                                              use_dropout, dropout_rates, use_batch_norm, momenta))\n","    \n","    # Convert tuples to dictionaries\n","    configurations = [\n","        {'optimizer_name': combo[0], 'batch_size': combo[1], 'learning_rate': combo[2], 'num_epochs': combo[3],\n","         'use_dropout': combo[4], 'dropout_rate': combo[5], 'use_batch_norm': combo[6], 'momentum': combo[7]}\n","        for combo in all_combinations if combo[0] == 'sgd' or combo[7] == None  # Adjusting momentum for optimizers\n","    ]\n","\n","    return configurations\n","\n","def sample_configurations(num_samples):\n","    configurations = generate_configurations()\n","    sampled_configurations = random.sample(configurations, min(num_samples, len(configurations)))  # Sample without replacement\n","    return sampled_configurations\n","\n","# Example usage\n","N = 20  # Number of configurations you want\n","sampled_configurations = sample_configurations(N)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_model = None\n","best_accuracy = 0\n","best_config = None\n","\n","for config in sampled_configurations:\n","    print(f\"Training with configuration: {config}\")\n","    # Adjust data loader batch size based on configuration\n","    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n","\n","    # Model initialization\n","    model = DualBranch1DCNN(use_dropout=config['use_dropout'],\n","                             dropout_rate=config['dropout_rate'],\n","                             use_batch_norm=config['use_batch_norm']).to(device)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate']) if config['optimizer_name'] == 'adam' \\\n","                else optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n","\n","    # Assume implementation of train_and_validate function that returns validation accuracy\n","    val_accuracy = train_model(model, train_loader, val_loader, device, config)\n","\n","    # Save the best model\n","    if val_accuracy > best_accuracy:\n","        best_accuracy = val_accuracy    \n","        best_model = model\n","        best_config = config\n","        torch.save(model.state_dict(), f'best_model_config_{config[\"config_id\"]}.pth')\n","\n","print(f\"Best Model Config: {best_config}, with Validation Accuracy: {best_accuracy}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
