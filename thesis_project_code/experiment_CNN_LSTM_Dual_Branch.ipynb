{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Imports\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.model_selection import StratifiedShuffleSplit\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import random_split\n","from torchviz import make_dot\n","\n","# Setup device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Seed and data paths\n","SEED = 42\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","DATA_DIRECTORY = 'Data/hyperaktiv_with_controls/hyperaktiv_with_controls/'\n","VALID_IDs = [1, 3, 5, 7, 9, 11, 15, 19, 20, 21, 22, 23, 24, 27, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 68, 71, 73, 75, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 101, 104, 105]\n","\n","# Load demographic data\n","demographic_data = pd.read_csv(f'{DATA_DIRECTORY}patient_info.csv', sep=';')\n","demographic_data = demographic_data[demographic_data['ID'].isin(VALID_IDs)]\n","labels = demographic_data['ADHD'].values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot ADHD class balance\n","demographic_data['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()\n","\n","# Extract control and ADHD IDs\n","control_ids = demographic_data[demographic_data['ADHD'] == 0]['ID'].values\n","adhd_ids = demographic_data[demographic_data['ADHD'] == 1]['ID'].values\n","print(f'Number of control patients: {len(control_ids)}; IDS: {control_ids}')\n","print(f'Number of ADHD patients: {len(adhd_ids)}; IDS: {adhd_ids}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function for splitting dataset\n","def split_dataset(ids, labels, train_ratio=0.80, val_ratio=0.10, test_ratio=0.10, random_seed=42):\n","    splits = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=random_seed)\n","    remaining_ratio = 1.0 - test_ratio\n","    val_relative_ratio = val_ratio / remaining_ratio\n","    train_val_ids, test_ids = next(splits.split(ids, labels))\n","    train_val_split = StratifiedShuffleSplit(n_splits=1, test_size=val_relative_ratio, random_state=random_seed)\n","    train_ids, val_ids = next(train_val_split.split(ids[train_val_ids], labels[train_val_ids]))\n","\n","    train_ids = ids[train_ids]\n","    val_ids = ids[val_ids]\n","    test_ids = ids[test_ids]\n","\n","    return train_ids, val_ids, test_ids\n","\n","# Split dataset\n","train_ids, val_ids, test_ids = split_dataset(np.array(VALID_IDs), np.array(labels))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot class balance in split datasets\n","def plot_class_balance(data, title):\n","    data['ADHD'].value_counts().plot(kind='bar', title=title)\n","    plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","    plt.ylabel('Number of records')\n","    plt.show()\n","\n","demographic_data_train = demographic_data[demographic_data['ID'].isin(train_ids)]\n","plot_class_balance(demographic_data_train, 'ADHD class balance in the train dataset')\n","\n","demographic_data_test = demographic_data[demographic_data['ID'].isin(test_ids)]\n","plot_class_balance(demographic_data_test, 'ADHD class balance in the test dataset')\n","\n","demographic_data_val = demographic_data[demographic_data['ID'].isin(val_ids)]\n","plot_class_balance(demographic_data_val, 'ADHD class balance in the validation dataset')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load data\n","def load_data(sample, demographic_data):\n","    patients_data = {}\n","    for patient_id in sample:\n","        hrv_data = pd.read_csv(f'{DATA_DIRECTORY}/hrv_data/patient_hr_{patient_id}.csv', sep=';')\n","        activity_data = pd.read_csv(f'{DATA_DIRECTORY}/activity_data/patient_activity_{patient_id}.csv', sep=';')\n","        labels = demographic_data[demographic_data['ID'] == patient_id]['ADHD'].values[0]\n","\n","        df_hrv = pd.DataFrame(data=hrv_data).set_index('TIMESTAMP')\n","        df_activity = pd.DataFrame(data=activity_data).set_index('TIMESTAMP')\n","\n","        min_length = min(len(df_hrv), len(df_activity))\n","        df_hrv = df_hrv.iloc[:min_length]\n","        df_activity = df_activity.iloc[:min_length]\n","\n","        patients_data[patient_id] = {\n","            'hrv': df_hrv,\n","            'activity': df_activity,\n","            'adhd': labels\n","        }\n","    return patients_data\n","\n","train_data = load_data(train_ids, demographic_data=demographic_data_train)\n","test_data = load_data(test_ids, demographic_data=demographic_data_test)\n","val_data = load_data(val_ids, demographic_data=demographic_data_val)\n","\n","all_data = {\n","    'train': train_data,\n","    'val': val_data,\n","    'test': test_data\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print shapes\n","def print_shapes(data, label):\n","    print(f\"{label} Patients: \")\n","    for patient_id, patient_data in data.items():\n","        print(f'Patient ID: {patient_id}; HRV shape: {patient_data[\"hrv\"].shape}; Activity shape: {patient_data[\"activity\"].shape}')\n","    print(\"---------------\")\n","\n","print_shapes(all_data['train'], \"Train\")\n","print_shapes(all_data['val'], \"Validation\")\n","print_shapes(all_data['test'], \"Test\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Data segmentation and normalization\n","def segment_data(data, window_size, step_size):\n","    segments = []\n","    for start in range(0, len(data) - window_size + 1, step_size):\n","        segment = data[start:start + window_size]\n","        segments.append(segment)\n","    return segments\n","\n","def normalize_data(data):\n","    scaler = RobustScaler()\n","    scaled_data = scaler.fit_transform(data)\n","    return scaled_data\n","\n","def preprocessData_create_windows(config, data):\n","    window_size = config['window_size']\n","    step_size = window_size // 2\n","    processed_data = {}\n","\n","    for patient_id, patient_data in data.items():\n","        hrv_segments = segment_data(patient_data['hrv']['HRV'], window_size, step_size)\n","        activity_segments = segment_data(patient_data['activity']['ACTIVITY'], window_size, step_size)\n","        hrv_normalized = normalize_data(np.array(hrv_segments).reshape(-1, window_size)).reshape(-1, window_size)\n","        activity_normalized = normalize_data(np.array(activity_segments).reshape(-1, window_size)).reshape(-1, window_size)\n","        labels_repeated = np.repeat(patient_data['adhd'], len(hrv_normalized))\n","\n","        processed_data[patient_id] = {'hrv': hrv_normalized, 'activity': activity_normalized, 'labels': labels_repeated}\n","\n","    return processed_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dataset and model classes\n","class PatientDataset(Dataset):\n","    def __init__(self, patients_data):\n","        self.patients_data = patients_data\n","        self.patient_ids = list(patients_data.keys())\n","        self.data = [(patient_id, idx) for patient_id in self.patient_ids for idx in range(len(patients_data[patient_id]['hrv']))]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        patient_id, data_idx = self.data[idx]\n","        patient_data = self.patients_data[patient_id]\n","\n","        hrv_data = torch.tensor(patient_data['hrv'][data_idx], dtype=torch.float32)\n","        activity_data = torch.tensor(patient_data['activity'][data_idx], dtype=torch.float32)\n","        label = torch.tensor(patient_data['labels'][data_idx], dtype=torch.float32)\n","\n","        return hrv_data, activity_data, label\n","\n","class MultimodalADHDNet(nn.Module):\n","    def __init__(self, output_channels, hidden_size, num_classes, use_batch_norm=True, use_dropout=True, dropout_rate=0.5):\n","        super(MultimodalADHDNet, self).__init__()\n","        self.use_batch_norm = use_batch_norm\n","        self.use_dropout = use_dropout\n","\n","        # HRV data branch\n","        self.hrv_conv1 = nn.Conv1d(1, output_channels, kernel_size=3, padding=1)\n","        if self.use_batch_norm:\n","            self.hrv_bn1 = nn.BatchNorm1d(output_channels)\n","\n","        self.hrv_lstm = nn.LSTM(output_channels, hidden_size, batch_first=True)\n","\n","        # Activity data branch\n","        self.act_conv1 = nn.Conv1d(1, output_channels, kernel_size=3, padding=1)\n","        if self.use_batch_norm:\n","            self.act_bn1 = nn.BatchNorm1d(output_channels)\n","\n","        self.act_lstm = nn.LSTM(output_channels, hidden_size, batch_first=True)\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(2 * hidden_size, num_classes)\n","        if self.use_dropout:\n","            self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, hrv_data, act_data):\n","        hrv_x = hrv_data.unsqueeze(1)\n","        act_x = act_data.unsqueeze(1)\n","\n","        hrv_x = self.hrv_conv1(hrv_x)\n","        if self.use_batch_norm:\n","            hrv_x = self.hrv_bn1(hrv_x)\n","        hrv_x = F.relu(hrv_x)\n","        hrv_x, _ = self.hrv_lstm(hrv_x.permute(0, 2, 1))\n","        hrv_x = hrv_x[:, -1, :]\n","\n","        act_x = self.act_conv1(act_x)\n","        if self.use_batch_norm:\n","            act_x = self.act_bn1(act_x)\n","        act_x = F.relu(act_x)\n","        act_x, _ = self.act_lstm(act_x.permute(0, 2, 1))\n","        act_x = act_x[:, -1, :]\n","\n","        x = torch.cat((hrv_x, act_x), dim=1)\n","        if self.use_dropout:\n","            x = self.dropout(x)\n","        x = self.fc(x)\n","        return torch.sigmoid(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Configurations\n","configurations = [\n","    {'config_id': 1, 'optimizer_name': 'adam', 'output_channels': 10, 'window_size': 50, 'batch_size': 64, 'hidden_size': 64, 'num_classes': 1, 'learning_rate': 0.001, 'num_epochs': 10, 'use_dropout': True, 'dropout_rate': 0.5, 'use_batch_norm': True},\n","    {'config_id': 3, 'optimizer_name': 'adam', 'output_channels': 32, 'window_size': 70, 'batch_size': 32, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 10, 'use_dropout': True, 'dropout_rate': 0.1, 'use_batch_norm': True},\n","    {'config_id': 4, 'optimizer_name': 'adam', 'output_channels': 64, 'window_size': 80, 'batch_size': 32, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 10, 'use_dropout': True, 'dropout_rate': 0.1, 'use_batch_norm': True},\n","    {'config_id': 6, 'optimizer_name': 'sgd', 'output_channels': 3, 'window_size': 10, 'batch_size': 16, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.001, 'num_epochs': 10, 'use_dropout': True, 'dropout_rate': 0.5, 'use_batch_norm': True, 'momentum': 0.9},\n","    {'config_id': 13, 'optimizer_name': 'sgd', 'output_channels': 5, 'window_size': 20, 'batch_size': 32, 'hidden_size': 64, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 10, 'use_dropout': True, 'dropout_rate': 0.3, 'use_batch_norm': True, 'momentum': 0.8},\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Metrics calculation\n","def calculate_metrics(outputs, labels):\n","    predicted = outputs.detach().round()\n","    accuracy = accuracy_score(labels.cpu().numpy(), predicted.cpu().numpy())\n","    precision = precision_score(labels.cpu().numpy(), predicted.cpu().numpy(), zero_division=0)\n","    recall = recall_score(labels.cpu().numpy(), predicted.cpu().numpy(), zero_division=0)\n","    f1 = f1_score(labels.cpu().numpy(), predicted.cpu().numpy(), zero_division=0)\n","    return accuracy, precision, recall, f1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training and validation\n","def train_epoch(model, data_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0\n","    total_batches = 0\n","\n","    for hrv_data, activity_data, labels in data_loader:\n","        hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(hrv_data, activity_data).squeeze(1)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * labels.size(0)\n","        accuracy, precision, recall, f1 = calculate_metrics(outputs, labels)\n","        total_accuracy += accuracy\n","        total_precision += precision\n","        total_recall += recall\n","        total_f1 += f1\n","        total_batches += 1\n","\n","    average_loss = running_loss / len(data_loader.dataset)\n","    average_accuracy = total_accuracy / total_batches\n","    average_precision = total_precision / total_batches\n","    average_recall = total_recall / total_batches\n","    average_f1 = total_f1 / total_batches\n","\n","    return average_loss, average_accuracy, average_precision, average_recall, average_f1\n","\n","def validate_epoch(model, data_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0\n","    total_batches = 0\n","\n","    with torch.no_grad():\n","        for hrv_data, activity_data, labels in data_loader:\n","            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device)\n","            outputs = model(hrv_data, activity_data).squeeze(1)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item() * labels.size(0)\n","            accuracy, precision, recall, f1 = calculate_metrics(outputs, labels)\n","            total_accuracy += accuracy\n","            total_precision += precision\n","            total_recall += recall\n","            total_f1 += f1\n","            total_batches += 1\n","\n","    average_loss = running_loss / len(data_loader.dataset)\n","    average_accuracy = total_accuracy / total_batches\n","    average_precision = total_precision / total_batches\n","    average_recall = total_recall / total_batches\n","    average_f1 = total_f1 / total_batches\n","\n","    return average_loss, average_accuracy, average_precision, average_recall, average_f1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Testing\n","def test_model(model, data_loader, device):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for hrv_data, activity_data, labels in data_loader:\n","            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device)\n","            outputs = model(hrv_data, activity_data)\n","            all_outputs.append(outputs.cpu().numpy())\n","            all_labels.append(labels.cpu().numpy())\n","\n","    all_outputs = np.concatenate(all_outputs)\n","    all_labels = np.concatenate(all_labels)\n","\n","    accuracy, precision, recall, f1 = calculate_metrics(torch.tensor(all_outputs), torch.tensor(all_labels))\n","\n","    try:\n","        roc_auc = roc_auc_score(all_labels, all_outputs)\n","    except ValueError:\n","        roc_auc = float('nan')\n","\n","    return accuracy, precision, recall, f1, roc_auc, all_outputs, all_labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Experiment runner\n","def run_experiment(config, all_data):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = MultimodalADHDNet(\n","        output_channels=config['output_channels'],\n","        hidden_size=config['hidden_size'],\n","        num_classes=config['num_classes'],\n","        use_batch_norm=config['use_batch_norm'],\n","        use_dropout=config['use_dropout'],\n","        dropout_rate=0.5\n","    ).to(device)\n","\n","    if config['optimizer_name'] == 'adam':\n","        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n","    elif config['optimizer_name'] == 'sgd':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n","\n","    criterion = nn.BCELoss()\n","\n","    train_data = preprocessData_create_windows(config, all_data['train'])\n","    val_data = preprocessData_create_windows(config, all_data['val'])\n","    test_data = preprocessData_create_windows(config, all_data['test'])\n","\n","    train_loader = DataLoader(PatientDataset(train_data), batch_size=config['batch_size'], shuffle=True)\n","    val_loader = DataLoader(PatientDataset(val_data), batch_size=config['batch_size'], shuffle=False)\n","    test_loader = DataLoader(PatientDataset(test_data), batch_size=config['batch_size'], shuffle=False)\n","\n","    final_train_metrics = {}\n","    final_val_metrics = {}\n","    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n","\n","    for epoch in range(config['num_epochs']):\n","        train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n","        val_loss, val_acc, val_prec, val_rec, val_f1 = validate_epoch(model, val_loader, criterion, device)\n","\n","        history['train_loss'].append(train_loss)\n","        history['train_acc'].append(train_acc)\n","        history['val_loss'].append(val_loss)\n","        history['val_acc'].append(val_acc)\n","\n","        if epoch == config['num_epochs'] - 1:\n","            final_train_metrics = {\n","                'Train Loss': round(train_loss, 2),\n","                'Train Accuracy': round(train_acc, 2),\n","                'Train Precision': round(train_prec, 2),\n","                'Train Recall': round(train_rec, 2),\n","                'Train F1': round(train_f1, 2)\n","            }\n","            final_val_metrics = {\n","                'Val Loss': round(val_loss, 2),\n","                'Val Accuracy': round(val_acc, 2),\n","                'Val Precision': round(val_prec, 2),\n","                'Val Recall': round(val_rec, 2),\n","                'Val F1': round(val_f1, 2)\n","            }\n","\n","    test_accuracy, test_precision, test_recall, test_f1, test_roc_auc, test_outputs, test_labels = test_model(model, test_loader, device)\n","    test_metrics = {\n","        'Test Accuracy': round(test_accuracy, 2),\n","        'Test Precision': round(test_precision, 2),\n","        'Test Recall': round(test_recall, 2),\n","        'Test F1': round(test_f1, 2),\n","        'Test ROC-AUC': round(test_roc_auc, 2)\n","    }\n","\n","    return history, {**final_train_metrics, **final_val_metrics, **test_metrics}, model, test_outputs, test_labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Experiment execution and results\n","results = []\n","config_histories = {}\n","best_val_acc = 0.0\n","best_model = None\n","best_config = None\n","best_history = None\n","best_test_outputs = None\n","best_test_labels = None\n","\n","for config in configurations:\n","    print(f\"Configuration: {config}\")\n","    history, test_results, model, test_outputs, test_labels = run_experiment(config, all_data)\n","    config_histories[config['config_id']] = history\n","    results.append({'config': config, **test_results})\n","\n","    val_acc = max(history['val_acc'])\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        best_model = model\n","        best_config = config\n","        best_history = history\n","        best_test_outputs = test_outputs\n","        best_test_labels = test_labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print and plot best results\n","print(f\"Best Configuration: {best_config}\")\n","print(f\"Test Metrics:\\n\")\n","for key, value in test_results.items():\n","    if key != 'config':\n","        print(f\"{key}: {value}\")\n","\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(best_history['train_loss'], label='Train Loss')\n","plt.plot(best_history['val_loss'], label='Validation Loss')\n","plt.title(f'Best Config {best_config[\"config_id\"]} - Loss Over Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(best_history['train_acc'], label='Train Accuracy')\n","plt.plot(best_history['val_acc'], label='Validation Accuracy')\n","plt.title(f'Best Config {best_config[\"config_id\"]} - Accuracy Over Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ROC-AUC curve\n","fpr, tpr, thresholds = roc_curve(best_test_labels, best_test_outputs)\n","roc_auc = roc_auc_score(best_test_labels, best_test_outputs)\n","\n","plt.figure()\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print model summary\n","print(best_model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save results\n","results_df = pd.DataFrame(results)\n","results_df.to_csv('results_batch05_best_tunned_windows.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Best model summary\n","best_model_summary = {\n","    \"Model Configuration\": best_config,\n","    \"Test Metrics\": {\n","        \"Test Accuracy\": test_results['Test Accuracy'],\n","        \"Test Precision\": test_results['Test Precision'],\n","        \"Test Recall\": test_results['Test Recall'],\n","        \"Test F1\": test_results['Test F1'],\n","        \"Test ROC-AUC\": test_results['Test ROC-AUC']\n","    }\n","}\n","\n","pd.set_option('display.max_columns', None)\n","print(pd.DataFrame(best_model_summary))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Configuration scores\n","print(\"\\nAll Configurations' Scores:\\n\")\n","results_summary_df = results_df.drop(columns=['config'])\n","print(results_summary_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Confusion matrix\n","conf_matrix = confusion_matrix(best_test_labels, (best_test_outputs > 0.5).astype(int))\n","ConfusionMatrixDisplay(confusion_matrix=conf_matrix).plot()\n","plt.title('Confusion Matrix for Best Model')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
