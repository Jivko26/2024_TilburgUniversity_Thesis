{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1. Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torchsummary import summary\n","from torchviz import make_dot\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","SEED = 42\n","\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","DATA_DIRECTORY = 'Data/hyperaktiv_with_controls/hyperaktiv_with_controls/'\n","VALID_IDs = [1, 3, 5, 7, 9, 11, 15, 19, 20, 21, 22, 23, 24, 27, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 68, 71, 73, 75, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 101, 104, 105]"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Load and Visualize Demographic Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["demographic_data = pd.read_csv(f'{DATA_DIRECTORY}patient_info.csv', sep=';')\n","\n","# Plot the balance of the ADHD class in the demographic_data for every record that has ID in VALID_IDs.\n","demographic_data_test_val = demographic_data[demographic_data['ID'].isin(VALID_IDs)]\n","labels = demographic_data_test_val['ADHD'].values\n","print(labels)\n","\n","demographic_data_test_val['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()\n","\n","control_ids = demographic_data_test_val[demographic_data_test_val['ADHD'] == 0]['ID'].values\n","adhd_ids = demographic_data_test_val[demographic_data_test_val['ADHD'] == 1]['ID'].values\n","\n","print(f'Number of control patients: {len(control_ids)}; IDS: {control_ids}')\n","print(f'Number of ADHD patients: {len(adhd_ids)}; IDS: {adhd_ids}')"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Split Data Into Train, Validation and Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_dataset(ids, labels, train_ratio=0.80, val_ratio=0.10, test_ratio=0.10, random_seed=42):\n","    splits = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=random_seed)\n","    remaining_ratio = 1.0 - test_ratio\n","    val_relative_ratio = val_ratio / remaining_ratio\n","    train_val_ids, test_ids = next(splits.split(ids, labels))\n","    train_val_split = StratifiedShuffleSplit(n_splits=1, test_size=val_relative_ratio, random_state=random_seed)\n","    train_ids, val_ids = next(train_val_split.split(ids[train_val_ids], labels[train_val_ids]))\n","\n","    train_ids = ids[train_ids]\n","    val_ids = ids[val_ids]\n","    test_ids = ids[test_ids]\n","\n","    return train_ids, val_ids, test_ids\n","\n","train_ids, val_ids, test_ids = split_dataset(np.array(VALID_IDs), np.array(labels))"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Visualize Class Balance in Each Split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["demographic_data_train = demographic_data_test_val[demographic_data_test_val['ID'].isin(train_ids)]\n","demographic_data_train['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the train dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()\n","\n","demographic_data_val = demographic_data_test_val[demographic_data_test_val['ID'].isin(val_ids)]\n","demographic_data_val['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the validation dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()\n","\n","demographic_data_test = demographic_data_test_val[demographic_data_test_val['ID'].isin(test_ids)]\n","demographic_data_test['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the test dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Define Data Loading and Preprocessing Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def scale_data(data):\n","    scaler = RobustScaler()\n","    return scaler.fit_transform(data.reshape(-1, 1)).flatten()\n","\n","def load_data(sample, demographic_data):\n","    patients_data = {}\n","\n","    for patient_id in sample:\n","        hrv_data = pd.read_csv(f'{DATA_DIRECTORY}hrv_data/patient_hr_{patient_id}.csv', sep=';')\n","        activity_data = pd.read_csv(f'{DATA_DIRECTORY}activity_data/patient_activity_{patient_id}.csv', sep=';')\n","        labels = demographic_data[demographic_data['ID'] == patient_id]['ADHD'].values[0]\n","\n","        hrv_data['TIMESTAMP'] = pd.to_datetime(hrv_data['TIMESTAMP'], errors='coerce')\n","        activity_data['TIMESTAMP'] = pd.to_datetime(activity_data['TIMESTAMP'], errors='coerce')\n","\n","        df_hrv = pd.DataFrame(data=hrv_data).set_index('TIMESTAMP')\n","        df_activity = pd.DataFrame(data=activity_data).set_index('TIMESTAMP')\n","\n","        df_hrv = df_hrv.resample('1S').mean()\n","        df_activity = df_activity.resample('1S').mean()\n","\n","        df_hrv['HRV'] = df_hrv['HRV'].fillna(method='ffill')\n","        df_activity['ACTIVITY'] = df_activity['ACTIVITY'].fillna(method='ffill')\n","\n","        hrv_series = scale_data(df_hrv['HRV'].values)\n","        activity_series = scale_data(df_activity['ACTIVITY'].values)\n","\n","        patients_data[patient_id] = {\n","            'hrv': hrv_series,\n","            'activity': activity_series,\n","            'adhd': labels\n","        }\n","\n","    return patients_data\n","\n","train_data = load_data(train_ids, demographic_data=demographic_data_train)\n","val_data = load_data(val_ids, demographic_data=demographic_data_val)\n","test_data = load_data(test_ids, demographic_data=demographic_data_test)\n","\n","all_data = {\n","    'train': train_data,\n","    'val': val_data,\n","    'test': test_data,\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Truncate and Pad Sequences"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def truncate_and_pad_sequences(sequence, max_length):\n","    sequence = sequence[:max_length]\n","    if len(sequence) < max_length:\n","        sequence = np.pad(sequence, (0, max_length - len(sequence)), 'constant', constant_values=0)\n","    return sequence\n","\n","def get_global_max_length(data):\n","    lengths = [len(data[pid]['hrv']) for pid in data]\n","    return int(np.percentile(lengths, 95))\n","\n","GLOBAL_MAX_LENGTH = max(get_global_max_length(train_data), get_global_max_length(val_data))\n","\n","for dataset in [all_data['train'], all_data['val'], all_data['test']]:\n","    for patient_id, data in dataset.items():\n","        data['hrv'] = truncate_and_pad_sequences(data['hrv'], GLOBAL_MAX_LENGTH)\n","        data['activity'] = truncate_and_pad_sequences(data['activity'], GLOBAL_MAX_LENGTH)"]},{"cell_type":"markdown","metadata":{},"source":["# 7. Define Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DualBranch1DCNN(nn.Module):\n","    def __init__(self, use_dropout=False, dropout_rate=0.5, use_batch_norm=False):\n","        super(DualBranch1DCNN, self).__init__()\n","        kernel_size = 5\n","        stride = 2\n","        pool_size = 2\n","        self.hrv_branch = self.build_branch(use_dropout, dropout_rate, use_batch_norm, kernel_size, stride, pool_size)\n","        self.activity_branch = self.build_branch(use_dropout, dropout_rate, use_batch_norm, kernel_size, stride, pool_size)\n","\n","        self.final_layers = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU()\n","        )\n","        if use_batch_norm:\n","            self.final_layers.add_module('final_batch_norm', nn.BatchNorm1d(128))\n","        if use_dropout:\n","            self.final_layers.add_module('final_dropout', nn.Dropout(dropout_rate))\n","        self.final_layers.add_module('final_output', nn.Linear(128, 1))\n","\n","    def build_branch(self, use_dropout, dropout_rate, use_batch_norm, kernel_size, stride, pool_size):\n","        layers = [\n","            nn.Conv1d(1, 16, kernel_size, stride=stride),\n","            nn.ReLU(),\n","            nn.AvgPool1d(pool_size),\n","            nn.Conv1d(16, 32, kernel_size, stride=stride),\n","            nn.ReLU(),\n","            nn.AvgPool1d(pool_size),\n","            nn.Flatten(),\n","            nn.Linear(32 * self.calculate_output_length(GLOBAL_MAX_LENGTH, kernel_size, stride, pool_size), 128),\n","            nn.ReLU()\n","        ]\n","        if use_batch_norm:\n","            layers.append(nn.BatchNorm1d(128))\n","        if use_dropout:\n","            layers.append(nn.Dropout(dropout_rate))\n","        return nn.Sequential(*layers)\n","\n","    def calculate_output_length(self, input_length, kernel_size, stride, pool_size):\n","        output_length = input_length\n","        for _ in range(2):\n","            output_length = (output_length - (kernel_size - 1) - 1) // stride + 1\n","            output_length = (output_length - (pool_size - 1) - 1) // pool_size + 1\n","        return output_length\n","\n","    def forward(self, hrv_data, activity_data):\n","        hrv_features = self.hrv_branch(hrv_data)\n","        activity_features = self.activity_branch(activity_data)\n","        combined_features = torch.cat((hrv_features, activity_features), dim=1)\n","        output = self.final_layers(combined_features)\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["# 8. Create Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ADHDData(Dataset):\n","    def __init__(self, data):\n","        self.hrv_data = [torch.tensor(data[pid]['hrv'], dtype=torch.float32).unsqueeze(0) for pid in data]\n","        self.activity_data = [torch.tensor(data[pid]['activity'], dtype=torch.float32).unsqueeze(0) for pid in data]\n","        self.labels = [data[pid]['adhd'] for pid in data]\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, index):\n","        return self.hrv_data[index], self.activity_data[index], self.labels[index]\n","\n","def pad_collate_fn(batch):\n","    hrv_data, activity_data, labels = zip(*batch)\n","    hrv_data_padded = pad_sequence(hrv_data, batch_first=True, padding_value=0)\n","    activity_data_padded = pad_sequence(activity_data, batch_first=True, padding_value=0)\n","    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n","    return hrv_data_padded, activity_data_padded, labels_tensor\n","\n","train_dataset = ADHDData(all_data['train'])\n","val_dataset = ADHDData(all_data['val'])\n","test_dataset = ADHDData(all_data['test'])\n","\n","print(f'Train dataset size: {len(train_dataset)}')\n","print(f'Validation dataset size: {len(val_dataset)}')\n","print(f'Test dataset size: {len(test_dataset)}')"]},{"cell_type":"markdown","metadata":{},"source":["# 9. Define Training And Validaiton Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(model, train_loader, val_loader, device, config):\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate']) if config['optimizer_name'] == 'adam' \\\n","                else optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config.get('momentum', 0.9))\n","\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    best_val_accuracy = 0\n","    best_metrics = {}\n","\n","    for epoch in range(config['num_epochs']):\n","        model.train()\n","        total = 0\n","        correct = 0\n","        train_loss = 0\n","\n","        for hrv_data, activity_data, labels in train_loader:\n","            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device).float()\n","\n","            optimizer.zero_grad()\n","            outputs = torch.sigmoid(model(hrv_data, activity_data))\n","            loss = criterion(outputs, labels.unsqueeze(1))\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            predicted = outputs.round()\n","            total += labels.size(0)\n","            correct += (predicted == labels.unsqueeze(1)).sum().item()\n","\n","        train_accuracy = 100 * correct / total\n","        train_losses.append(train_loss / len(train_loader))\n","        train_accuracies.append(train_accuracy)\n","\n","        val_loss, val_accuracy = validate_model(model, val_loader, criterion, device)\n","        val_losses.append(val_loss)\n","        val_accuracies.append(val_accuracy)\n","\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            best_metrics = {\n","                'train_loss': train_losses,\n","                'val_loss': val_losses,\n","                'train_accuracy': train_accuracies,\n","                'val_accuracy': val_accuracies\n","            }\n","            torch.save(model.state_dict(), 'best_model.pth')\n","            print(\"Best model updated.\")\n","\n","    return best_metrics\n","\n","def validate_model(model, val_loader, criterion, device):\n","    model.eval()\n","    val_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for hrv_data, activity_data, labels in val_loader:\n","            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device).float()\n","            outputs = torch.sigmoid(model(hrv_data, activity_data))\n","            loss = criterion(outputs, labels.unsqueeze(1))\n","\n","            val_loss += loss.item()\n","            predicted = outputs.round()\n","            correct += (predicted == labels.unsqueeze(1)).sum().item()\n","            total += labels.size(0)\n","\n","    accuracy = 100 * correct / total\n","    return val_loss / len(val_loader), accuracy"]},{"cell_type":"markdown","metadata":{},"source":["# 10. Generate and Sample Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","import itertools\n","\n","def generate_configurations():\n","    optimizer_names = ['adam', 'sgd']\n","    batch_sizes = [16, 32, 64]\n","    learning_rates = [0.001, 0.01]\n","    num_epochs = [20, 30]\n","    use_dropout = [True, False]\n","    dropout_rates = [0.2, 0.5]\n","    use_batch_norm = [True, False]\n","    momenta = [0.8, 0.9]\n","\n","    all_combinations = list(itertools.product(optimizer_names, batch_sizes, learning_rates, num_epochs,\n","                                              use_dropout, dropout_rates, use_batch_norm, momenta))\n","\n","    configurations = [\n","        {'optimizer_name': combo[0], 'batch_size': combo[1], 'learning_rate': combo[2], 'num_epochs': combo[3],\n","         'use_dropout': combo[4], 'dropout_rate': combo[5], 'use_batch_norm': combo[6], 'momentum': combo[7]}\n","        for combo in all_combinations if combo[0] == 'sgd' or combo[7] == None\n","    ]\n","\n","    return configurations\n","\n","def sample_configurations(num_samples):\n","    configurations = generate_configurations()\n","    sampled_configurations = random.sample(configurations, min(num_samples, len(configurations)))\n","    return sampled_configurations"]},{"cell_type":"markdown","metadata":{},"source":["# 11. Train Models with Sampled Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["N = 10\n","sampled_configurations = sample_configurations(N)\n","best_model = None\n","best_metrics = None\n","best_accuracy = 0\n","best_config = None\n","\n","results = []\n","\n","for config in sampled_configurations:\n","    print(f\"Training with configuration: {config}\")\n","    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=pad_collate_fn)\n","    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=pad_collate_fn)\n","\n","    model = DualBranch1DCNN(\n","        use_dropout=config['use_dropout'],\n","        dropout_rate=config['dropout_rate'],\n","        use_batch_norm=config['use_batch_norm']).to(device)\n","\n","    metrics = train_model(model, train_loader, val_loader, device, config)\n","\n","    current_val_accuracy = metrics['val_accuracy'][-1]\n","    if current_val_accuracy > best_accuracy:\n","        best_accuracy = current_val_accuracy\n","        best_model = model\n","        best_metrics = metrics\n","        best_config = config\n","        torch.save(model.state_dict(), f'best_model.pth')\n","        print(f\"New best model saved with config ID {config} and validation accuracy {best_accuracy}\")\n","\n","    results.append({\n","        'config': config,\n","        'train_loss': metrics['train_loss'][-1],\n","        'val_loss': metrics['val_loss'][-1],\n","        'train_accuracy': metrics['train_accuracy'][-1],\n","        'val_accuracy': metrics['val_accuracy'][-1]\n","    })\n","\n","print(f\"Best Model Config: {best_config}, with Best Validation Accuracy: {best_accuracy}\")"]},{"cell_type":"markdown","metadata":{},"source":["# 12. Plot Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_metrics(metrics):\n","    epochs = range(len(metrics['train_loss']))\n","    plt.figure(figsize=(10, 5))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs, metrics['train_loss'], label='Train Loss')\n","    plt.plot(epochs, metrics['val_loss'], label='Validation Loss')\n","    plt.title('Loss over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs, metrics['train_accuracy'], label='Train Accuracy')\n","    plt.plot(epochs, metrics['val_accuracy'], label='Validation Accuracy')\n","    plt.title('Accuracy over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy (%)')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_metrics(best_metrics)"]},{"cell_type":"markdown","metadata":{},"source":["# 13. Test Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_loader = DataLoader(test_dataset, batch_size=best_config['batch_size'], shuffle=False, collate_fn=pad_collate_fn)\n","\n","def test_model(model, test_loader, device):\n","    criterion = nn.BCELoss()\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    TP = 0\n","    FP = 0\n","    FN = 0\n","\n","    all_predictions = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for hrv_data, activity_data, labels in test_loader:\n","            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device).float()\n","            outputs = torch.sigmoid(model(hrv_data, activity_data))\n","            loss = criterion(outputs, labels.unsqueeze(1))\n","            test_loss += loss.item()\n","            predicted = outputs.round()\n","\n","            correct += (predicted == labels.unsqueeze(1)).sum().item()\n","            total += labels.size(0)\n","\n","            TP += (predicted * labels.unsqueeze(1)).sum().item()\n","            FP += (predicted * (1 - labels.unsqueeze(1))).sum().item()\n","            FN += ((1 - predicted) * labels.unsqueeze(1)).sum().item()\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = correct / total\n","    precision = TP / (TP + FP) if TP + FP > 0 else 0\n","    recall = TP / (TP + FN) if TP + FN > 0 else 0\n","    avg_loss = test_loss / len(test_loader)\n","\n","    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%, Precision: {precision:.2f}, Recall: {recall:.2f}')\n","    return avg_loss, accuracy, precision, recall, all_labels, all_predictions\n","\n","test_loss, test_accuracy, precision, recall, labels, predictions = test_model(best_model, test_loader, device)\n","print(test_loss, test_accuracy, precision, recall)"]},{"cell_type":"markdown","metadata":{},"source":["# 14. Plot ROC-AUC Curve"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_roc_auc(labels, predictions):\n","    fpr, tpr, _ = roc_curve(labels, predictions)\n","    roc_auc = auc(fpr, tpr)\n","\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","plot_roc_auc(labels, predictions)"]},{"cell_type":"markdown","metadata":{},"source":["# 15. Visualize Model Architecture and Summary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_hrv = torch.randn(1, 1, GLOBAL_MAX_LENGTH).to(device)\n","x_activity = torch.randn(1, 1, GLOBAL_MAX_LENGTH).to(device)\n","y = best_model(x_hrv, x_activity)\n","make_dot(y, params=dict(best_model.named_parameters())).render(\"model_architecture\", format=\"png\")\n","\n","summary(best_model, [(1, GLOBAL_MAX_LENGTH), (1, GLOBAL_MAX_LENGTH)])"]},{"cell_type":"markdown","metadata":{},"source":["# 16. Classification Report and Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predicted_labels = np.round(predictions)\n","report = classification_report(labels, predicted_labels, target_names=['Control', 'ADHD'])\n","print(report)\n","\n","cm = confusion_matrix(labels, predicted_labels)\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=['Control', 'ADHD'], yticklabels=['Control', 'ADHD'])\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 17. Save Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_df = pd.DataFrame(results)\n","results_df.to_csv('1d_CNN_AVGPool_results.csv', index=False)\n","\n","best_model_summary = {\n","    \"Model Configuration\": best_config,\n","    \"Test Metrics\": {\n","        \"Test Accuracy\": test_accuracy,\n","        \"Test Precision\": precision,\n","        \"Test Recall\": recall,\n","        \"Test Loss\": test_loss\n","    }\n","}\n","\n","pd.set_option('display.max_columns', None)\n","print(pd.DataFrame(best_model_summary))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
