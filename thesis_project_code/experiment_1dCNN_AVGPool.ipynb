{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = './Data/hyperaktiv_with_controls/hyperaktiv_with_controls/'\n",
    "VALID_IDs = [1, 3, 5, 11, 15, 19, 20, 21, 22, 23, 24, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 68, 71, 73, 75, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 101, 104, 105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_data = pd.read_csv(f'{DATA_DIRECTORY}patient_info.csv', sep=';')\n",
    "#plot the balance of the ADHD class in the demographic_data for every record that has ID in VALID_IDs. Insert labels and make it more appealing\n",
    "demographic_data = demographic_data[demographic_data['ID'].isin(VALID_IDs)]\n",
    "# Extract labels for these IDs\n",
    "labels = demographic_data['ADHD'].values\n",
    "\n",
    "# Output the labels to verify\n",
    "print(labels)\n",
    "\n",
    "demographic_data['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the dataset')\n",
    "plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n",
    "plt.ylabel('Number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to noe the IDS of the control and ADHD patients\n",
    "control_ids = demographic_data[demographic_data['ADHD'] == 0]['ID'].values\n",
    "adhd_ids = demographic_data[demographic_data['ADHD'] == 1]['ID'].values\n",
    "\n",
    "print(f'Number of control patients: {len(control_ids)}; IDS: {control_ids}')\n",
    "print(f'Number of ADHD patients: {len(adhd_ids)}; IDS: {adhd_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_data(data):\n",
    "    scaler = RobustScaler()\n",
    "    return scaler.fit_transform(data.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Extract hrv_data, activity_data, and labels from the batch\n",
    "    hrv_data = [item[0] for item in batch]\n",
    "    activity_data = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "\n",
    "    # Pad sequences so they are all the same length within each batch\n",
    "    # Find the maximum length of any sequence in the batch for hrv_data and activity_data\n",
    "    max_length_hrv = max([s.size(1) for s in hrv_data])  # size(1) because the dimension 0 is the batch dimension\n",
    "    max_length_activity = max([s.size(1) for s in activity_data])\n",
    "\n",
    "    # Pad all sequences to the maximum length found\n",
    "    hrv_data = [F.pad(seq, (0, max_length_hrv - seq.size(1))) for seq in hrv_data]\n",
    "    activity_data = [F.pad(seq, (0, max_length_activity - seq.size(1))) for seq in activity_data]\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    hrv_data = torch.stack(hrv_data)\n",
    "    activity_data = torch.stack(activity_data)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    return hrv_data, activity_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.keys = list(data.keys())  # Store the keys\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use the stored keys to handle indexing\n",
    "        key = self.keys[idx]\n",
    "        item = self.data[key]\n",
    "        hrv_data = torch.tensor(item['hrv'], dtype=torch.float32).unsqueeze(0)  # Ensure channel dimension is added\n",
    "        activity_data = torch.tensor(item['activity'], dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor(item['adhd'], dtype=torch.float32)\n",
    "        return hrv_data, activity_data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sample, demographic_data, is_train=True):\n",
    "    data = {}\n",
    "    suffix = 'train' if is_train else 'test'\n",
    "    for patient_id in sample:\n",
    "        hrv_data = pd.read_csv(f'{DATA_DIRECTORY}/hrv_data_{suffix}/patient_hr_{patient_id}.csv', sep=';')['HRV'].values\n",
    "        activity_data = pd.read_csv(f'{DATA_DIRECTORY}/activity_data_{suffix}/patient_activity_{patient_id}.csv', sep=';')['ACTIVITY'].values\n",
    "        label = demographic_data[demographic_data['ID'] == patient_id]['ADHD'].values[0]\n",
    "\n",
    "        hrv_scaled = scale_data(hrv_data)\n",
    "        activity_scaled = scale_data(activity_data)\n",
    "\n",
    "        data[patient_id] = {'hrv': hrv_scaled, 'activity': activity_scaled, 'adhd': label}\n",
    "    return data\n",
    "\n",
    "\n",
    "all_data = load_data(VALID_IDs, demographic_data=demographic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualBranch1DCNN(nn.Module):\n",
    "    def __init__(self, num_channels, use_dropout=False, dropout_rate=0.5, use_batch_norm=False):\n",
    "        super(DualBranch1DCNN, self).__init__()\n",
    "        self.hrv_branch = self.create_branch(num_channels, use_dropout, dropout_rate, use_batch_norm)\n",
    "        self.activity_branch = self.create_branch(num_channels, use_dropout, dropout_rate, use_batch_norm)\n",
    "        # Assuming both branches are concatenated, thus doubling the feature maps\n",
    "        final_input_features = 2 * num_channels  # Adjust according to actual output of branches\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(final_input_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128) if use_batch_norm else nn.Identity(),  # Ensure this matches the output of the Linear layer\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def create_branch(self, num_channels, use_dropout, dropout_rate, use_batch_norm):\n",
    "        layers = [\n",
    "            nn.Conv1d(1, num_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_channels) if use_batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_channels) if use_batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128) if use_batch_norm else nn.Identity(),  # Correct placement and dimension\n",
    "            nn.Dropout(dropout_rate) if use_dropout else nn.Identity()\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, hrv_data, activity_data):\n",
    "        hrv_features = self.hrv_branch(hrv_data)\n",
    "        activity_features = self.activity_branch(activity_data)\n",
    "        combined_features = torch.cat((hrv_features, activity_features), dim=1)\n",
    "        output = self.final_layers(combined_features)\n",
    "        return output\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, config):\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate']) if config['optimizer_name'] == 'adam' \\\n",
    "                else torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config.get('momentum', 0.9))\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    best_metrics = {}\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        train_loss = 0\n",
    "\n",
    "        for hrv_data, activity_data, labels in train_loader:\n",
    "            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(hrv_data, activity_data)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predicted = torch.sigmoid(outputs).round()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_accuracy = validate_model(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{config[\"num_epochs\"]}:')\n",
    "        print(f'  Training Loss: {train_loss / len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "        print(f'  Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Save best model based on validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_metrics = {\n",
    "                'train_loss': train_losses,\n",
    "                'val_loss': val_losses,\n",
    "                'train_accuracy': train_accuracies,\n",
    "                'val_accuracy': val_accuracies\n",
    "            }\n",
    "            print(\" Best model updated.\")\n",
    "\n",
    "    return best_metrics\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for hrv_data, activity_data, labels in val_loader:\n",
    "            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device).float()\n",
    "            outputs = model(hrv_data, activity_data)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predicted = torch.sigmoid(outputs).round()\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return val_loss / len(val_loader), accuracy\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validate_model(model_class, configurations, all_data, k=5, device='cpu'):\n",
    "    patient_ids = list(all_data.keys())\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_config = None\n",
    "\n",
    "    for train_idx, val_idx in kf.split(patient_ids):\n",
    "        train_ids = [patient_ids[i] for i in train_idx]  # Convert indices to patient IDs\n",
    "        val_ids = [patient_ids[i] for i in val_idx]      # Convert indices to patient IDs\n",
    "\n",
    "        train_data = {pid: all_data[pid] for pid in train_ids}\n",
    "        val_data = {pid: all_data[pid] for pid in val_ids}\n",
    "\n",
    "        for config in configurations:\n",
    "            print(f\"Training with configuration: {config}\")\n",
    "            model = model_class(config['num_channels'], use_dropout=config['use_dropout'], dropout_rate=config['dropout_rate'], use_batch_norm=config['use_batch_norm']).to(device)\n",
    "            \n",
    "            # Loaders\n",
    "            train_loader = DataLoader(VariableLengthDataset(train_data), batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "            val_loader = DataLoader(VariableLengthDataset(val_data), batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "            metrics = train_model(model, train_loader, val_loader, device, config)\n",
    "            current_val_accuracy = metrics['val_accuracy'][-1]\n",
    "\n",
    "            if current_val_accuracy > best_accuracy:\n",
    "                best_accuracy = current_val_accuracy\n",
    "                best_model = model\n",
    "                best_config = config\n",
    "                print(f\"New best model found with accuracy: {best_accuracy:.2f}\")\n",
    "\n",
    "    return best_model, best_config, best_accuracy\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "def generate_configurations():\n",
    "    # Define your hyperparameter space\n",
    "    optimizer_names = ['adam', 'sgd']\n",
    "    batch_sizes = [16, 32, 64, 128]\n",
    "    learning_rates = [0.001, 0.01, 0.05]\n",
    "    num_epochs = [20, 30, 40, 50, 60, 70]\n",
    "    use_dropout = [True, False]\n",
    "    dropout_rates = [0.1, 0.2, 0.3, 0.5]\n",
    "    use_batch_norm = [True, False]\n",
    "    momenta = [0.8, 0.9]  # Only for 'sgd'\n",
    "    num_channels = [16, 32, 64, 128]\n",
    "\n",
    "    # Cartesian product to generate all combinations\n",
    "    all_combinations = list(itertools.product(optimizer_names, batch_sizes, learning_rates, num_epochs,\n",
    "                                              use_dropout, dropout_rates, use_batch_norm, momenta, num_channels))\n",
    "\n",
    "    # Convert tuples to dictionaries\n",
    "    configurations = [\n",
    "        {'optimizer_name': combo[0], 'batch_size': combo[1], 'learning_rate': combo[2], 'num_epochs': combo[3],\n",
    "         'use_dropout': combo[4], 'dropout_rate': combo[5], 'use_batch_norm': combo[6], 'momentum': combo[7], 'num_channels': combo[8]}\n",
    "        for combo in all_combinations if combo[0] == 'sgd' or combo[7] == None  # Adjusting momentum for optimizers\n",
    "    ]\n",
    "\n",
    "    return configurations\n",
    "\n",
    "def sample_configurations(num_samples):\n",
    "    configurations = generate_configurations()\n",
    "    sampled_configurations = random.sample(configurations, min(num_samples, len(configurations)))  # Sample without replacement\n",
    "    return sampled_configurations\n",
    "\n",
    "# Example usage\n",
    "N = 20  # Number of configurations you want\n",
    "sampled_configurations = sample_configurations(N)\n",
    "best_model, best_config, best_accuracy = cross_validate_model(DualBranch1DCNN, sampled_configurations, all_data, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "print(f\"Best accuracy: {best_accuracy:.2f}\")\n",
    "print(f\"Best configuration: {best_config}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
