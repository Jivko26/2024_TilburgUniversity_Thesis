{"cells":[{"cell_type":"markdown","metadata":{"id":"amMZWTSenDs5"},"source":["# 1. Initial Steps and Data Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEnaHANQAp3p"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import random_split"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715000176546,"user":{"displayName":"Jivko Parapanov","userId":"13549335107747025520"},"user_tz":-120},"id":"5XnG1MZ_As85","outputId":"b4a4ed4d-7869-43b4-f386-44c4a897113d"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","SEED = 42\n","\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnUUpJqRAwPj"},"outputs":[],"source":["DATA_DIRECTORY = '/content/drive/MyDrive/Thesis/Data/hyperaktiv_with_controls/hyperaktiv_with_controls/'\n","VALID_IDs = [1, 3, 5, 7, 9, 11, 15, 19, 20, 21, 22, 23, 24, 27, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 68, 71, 73, 75, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 101, 104, 105]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":3923,"status":"ok","timestamp":1715000180463,"user":{"displayName":"Jivko Parapanov","userId":"13549335107747025520"},"user_tz":-120},"id":"j5CUst8WAzKJ","outputId":"6070eef6-cfba-490b-f0ee-bf283e538f87"},"outputs":[],"source":["demographic_data = pd.read_csv(f'{DATA_DIRECTORY}patient_info.csv', sep=';')\n","#plot the balance of the ADHD class in the demographic_data for every record that has ID in VALID_IDs. Insert labels and make it more appealing\n","demographic_data = demographic_data[demographic_data['ID'].isin(VALID_IDs)]\n","# Extract labels for these IDs\n","labels = demographic_data['ADHD'].values\n","\n","# Output the labels to verify\n","print(labels)\n","\n","demographic_data['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1715000180464,"user":{"displayName":"Jivko Parapanov","userId":"13549335107747025520"},"user_tz":-120},"id":"kj_rZbieA0qe","outputId":"872bfe9c-2675-4d41-f86c-08439d683d80"},"outputs":[],"source":["# I want to noe the IDS of the control and ADHD patients\n","control_ids = demographic_data[demographic_data['ADHD'] == 0]['ID'].values\n","adhd_ids = demographic_data[demographic_data['ADHD'] == 1]['ID'].values\n","\n","print(f'Number of control patients: {len(control_ids)}; IDS: {control_ids}')\n","print(f'Number of ADHD patients: {len(adhd_ids)}; IDS: {adhd_ids}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfLJYY58UeWH"},"outputs":[],"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","\n","def enhanced_split_dataset(ids, labels, train_ratio=0.80, val_ratio=0.10, test_ratio=0.10, random_seed=42):\n","    # Convert ratios to a useable format for StratifiedShuffleSplit\n","    splits = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=random_seed)\n","\n","    # Remaining ratio is for validation\n","    remaining_ratio = 1.0 - test_ratio\n","    val_relative_ratio = val_ratio / remaining_ratio\n","\n","    # First split to separate out the test set\n","    train_val_ids, test_ids = next(splits.split(ids, labels))\n","\n","    # Second split to separate out the validation set from the remaining train set\n","    train_val_split = StratifiedShuffleSplit(n_splits=1, test_size=val_relative_ratio, random_state=random_seed)\n","    train_ids, val_ids = next(train_val_split.split(ids[train_val_ids], labels[train_val_ids]))\n","\n","    # Convert indices to actual IDs\n","    train_ids = ids[train_ids]\n","    val_ids = ids[val_ids]\n","    test_ids = ids[test_ids]\n","\n","    return train_ids, val_ids, test_ids\n","\n","# Example usage:\n","# Assume labels is an array of labels corresponding to VALID_IDs in the same order\n","train_ids, val_ids, test_ids = enhanced_split_dataset(np.array(VALID_IDs), np.array(labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":582,"status":"ok","timestamp":1715000181031,"user":{"displayName":"Jivko Parapanov","userId":"13549335107747025520"},"user_tz":-120},"id":"OJr2s-TYBRXN","outputId":"e934e71b-502c-4697-efaf-37d10bc7a3f8"},"outputs":[],"source":["demographic_data_train = demographic_data[demographic_data['ID'].isin(train_ids)]\n","demographic_data_train['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the train dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()\n","\n","demographic_data_test = demographic_data[demographic_data['ID'].isin(test_ids)]\n","demographic_data_test['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the test dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()\n","\n","demographic_data_val = demographic_data[demographic_data['ID'].isin(val_ids)]\n","demographic_data_val['ADHD'].value_counts().plot(kind='bar', title='ADHD class balance in the valdiation dataset')\n","plt.xticks([0, 1], ['Control', 'ADHD'], rotation=0)\n","plt.ylabel('Number of records')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ss8WatgzBTVX"},"outputs":[],"source":["\n","def load_data(sample, demographic_data):\n","    patients_data = {}  # Dictionary to store data\n","\n","    for patient_id in sample:\n","        hrv_data = pd.read_csv(f'{DATA_DIRECTORY}/hrv_data/patient_hr_{patient_id}.csv', sep=';')\n","        activity_data = pd.read_csv(f'{DATA_DIRECTORY}/activity_data/patient_activity_{patient_id}.csv', sep=';')\n","        labels =  demographic_data[demographic_data['ID'] == patient_id]['ADHD'].values[0]  # Get the ADHD label for the patient\n","\n","    # # Convert TIMESTAMP to datetime\n","    #     hrv_data['TIMESTAMP'] = pd.to_datetime(hrv_data['TIMESTAMP'], errors='coerce')\n","    #     activity_data['TIMESTAMP'] = pd.to_datetime(activity_data['TIMESTAMP'], errors='coerce')\n","\n","    # Setting TIMESTAMP as index and checking for NaNs in data columns\n","        df_hrv = pd.DataFrame(data=hrv_data).set_index('TIMESTAMP')\n","        df_activity = pd.DataFrame(data=activity_data).set_index('TIMESTAMP')\n","\n","    # # Fill NaNs in HRV and Activity before resampling\n","    #     df_hrv['HRV'] = df_hrv['HRV'].fillna(method='ffill')  # Forward fill as an example\n","    #     df_activity['ACTIVITY'] = df_activity['ACTIVITY'].fillna(method='ffill')  # Forward fill as an example\n","\n","    # # Now resample\n","    #     df_hrv = df_hrv.resample('1T').mean()\n","    #     df_activity = df_activity.resample('1T').mean()\n","\n","    # Trim datasets to the same length\n","        min_length = min(len(df_hrv), len(df_activity))\n","        df_hrv = df_hrv.iloc[:min_length]\n","        df_activity = df_activity.iloc[:min_length]\n","\n","    # Store in dictionary\n","        patients_data[patient_id] = {\n","        'hrv': df_hrv,\n","        'activity': df_activity,\n","        'adhd': labels\n","        }\n","\n","    return patients_data\n","\n","\n","train_data = load_data(train_ids, demographic_data=demographic_data_train)\n","test_data = load_data(test_ids, demographic_data=demographic_data_test)\n","val_data = load_data(val_ids, demographic_data=demographic_data_val)\n","\n","all_data = {\n","    'train':  train_data,\n","    'val': val_data,\n","    'test': test_data\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1715000349782,"user":{"displayName":"Jivko Parapanov","userId":"13549335107747025520"},"user_tz":-120},"id":"hrHH5KX2Xi5Q","outputId":"c65dbd8c-bb88-458e-9100-4f215e99fcb6"},"outputs":[],"source":["# print shapes\n","print(\"Train Patients: \")\n","for patient_id, data in all_data['train'].items():\n","    print(f'Patient ID: {patient_id}; HRV shape: {data[\"hrv\"].shape}; Activity shape: {data[\"activity\"].shape}')\n","\n","print(\"---------------\")\n","print(\"Validation Patients: \")\n","# print shapes\n","for patient_id, data in all_data['val'].items():\n","    print(f'Patient ID: {patient_id}; HRV shape: {data[\"hrv\"].shape}; Activity shape: {data[\"activity\"].shape}')\n","\n","print(\"---------------\")\n","print(\"Test Patients: \")\n","# print shapes\n","for patient_id, data in all_data['test'].items():\n","    print(f'Patient ID: {patient_id}; HRV shape: {data[\"hrv\"].shape}; Activity shape: {data[\"activity\"].shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rKsLz0BdGG9u"},"outputs":[],"source":["\n","\n","def segment_data(data, window_size, step_size):\n","    segments = []\n","    for start in range(0, len(data) - window_size + 1, step_size):\n","        segment = data[start:start + window_size]\n","        segments.append(segment)\n","    return segments\n","\n","def normalize_data(data):\n","    scaler = RobustScaler()\n","    scaled_data = scaler.fit_transform(data)\n","    return scaled_data\n","\n","def preprocessData_create_windows(config, data):\n","    window_size = config['window_size']\n","    step_size = window_size // 2\n","    processed_data = {}\n","\n","    for patient_id, patient_data in data.items():\n","        # Segment and normalize HRV and activity data\n","        hrv_segments = segment_data(patient_data['hrv']['HRV'], window_size, step_size)\n","        activity_segments = segment_data(patient_data['activity']['ACTIVITY'], window_size, step_size)\n","        hrv_normalized = normalize_data(np.array(hrv_segments).reshape(-1, window_size)).reshape(-1, window_size)\n","        activity_normalized = normalize_data(np.array(activity_segments).reshape(-1, window_size)).reshape(-1, window_size)\n","        labels_repeated = np.repeat(patient_data['adhd'], len(hrv_normalized))\n","\n","        processed_data[patient_id] = {'hrv': hrv_normalized, 'activity': activity_normalized, 'labels': labels_repeated}\n","\n","    return processed_data\n","\n","def preprocessData_no_windows(config, data):\n","    processed_data = {}\n","\n","    for patient_id, patient_data in data.items():\n","        hrv_normalized = normalize_data(np.array(patient_data['hrv']['HRV']).reshape(-1, 1)).reshape(-1, 1)\n","        activity_normalized = normalize_data(np.array(patient_data['activity']['ACTIVITY']).reshape(-1, 1)).reshape(-1, 1)\n","        labels_repeated = np.repeat(patient_data['adhd'], len(hrv_normalized))\n","\n","        processed_data[patient_id] = {'hrv': hrv_normalized, 'activity': activity_normalized, 'labels': labels_repeated}\n","\n","    return processed_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akyeBPhUGMHp"},"outputs":[],"source":["class PatientDataset(Dataset):\n","    def __init__(self, patients_data):\n","        self.patients_data = patients_data\n","        self.patient_ids = list(patients_data.keys())\n","        self.data = [(patient_id, idx) for patient_id in self.patient_ids for idx in range(len(patients_data[patient_id]['hrv']))]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        patient_id, data_idx = self.data[idx]\n","        patient_data = self.patients_data[patient_id]\n","\n","        hrv_data = torch.tensor(patient_data['hrv'][data_idx], dtype=torch.float32)\n","        activity_data = torch.tensor(patient_data['activity'][data_idx], dtype=torch.float32)\n","        label = torch.tensor(patient_data['labels'][data_idx], dtype=torch.float32)\n","\n","        return hrv_data, activity_data, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BprrpL_1GQK8"},"outputs":[],"source":["class MultimodalADHDNet(nn.Module):\n","    def __init__(self, output_channels, hidden_size, num_classes, use_batch_norm=True, use_dropout=True, dropout_rate=0.5):\n","        super(MultimodalADHDNet, self).__init__()\n","        self.use_batch_norm = use_batch_norm\n","        self.use_dropout = use_dropout\n","\n","        # HRV data branch\n","        self.hrv_conv1 = nn.Conv1d(1, output_channels, kernel_size=3, padding=1)\n","        if self.use_batch_norm:\n","            self.hrv_bn1 = nn.BatchNorm1d(output_channels)\n","\n","        self.hrv_lstm = nn.LSTM(output_channels, hidden_size, batch_first=True)\n","\n","        # Activity data branch\n","        self.act_conv1 = nn.Conv1d(1, output_channels, kernel_size=3, padding=1)\n","        if self.use_batch_norm:\n","            self.act_bn1 = nn.BatchNorm1d(output_channels)\n","\n","        self.act_lstm = nn.LSTM(output_channels, hidden_size, batch_first=True)\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(2 * hidden_size, num_classes)\n","        if self.use_dropout:\n","            self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, hrv_data, act_data):\n","        # Ensure input data is correctly shaped [batch_size, 1, sequence_length]\n","        hrv_x = hrv_data.unsqueeze(1)  # Add a channel dimension if missing\n","        act_x = act_data.unsqueeze(1)  # Add a channel dimension if missing\n","\n","        hrv_x = self.hrv_conv1(hrv_x)\n","        if self.use_batch_norm:\n","            hrv_x = self.hrv_bn1(hrv_x)\n","        hrv_x = F.relu(hrv_x)\n","        hrv_x, _ = self.hrv_lstm(hrv_x.permute(0, 2, 1))\n","        hrv_x = hrv_x[:, -1, :]\n","\n","        act_x = self.act_conv1(act_x)\n","        if self.use_batch_norm:\n","            act_x = self.act_bn1(act_x)\n","        act_x = F.relu(act_x)\n","        act_x, _ = self.act_lstm(act_x.permute(0, 2, 1))\n","        act_x = act_x[:, -1, :]\n","\n","        x = torch.cat((hrv_x, act_x), dim=1)\n","        if self.use_dropout:\n","            x = self.dropout(x)\n","        x = self.fc(x)\n","        return torch.sigmoid(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNhbDTj7oXH7"},"outputs":[],"source":["configurations = [\n","    {'config_id': 1, 'optimizer_name': 'adam', 'output_channels': 10, 'window_size': 50, 'batch_size': 64, 'hidden_size': 64, 'num_classes': 1, 'learning_rate': 0.001, 'num_epochs': 100, 'use_dropout': True, 'dropout_rate': 0.5, 'use_batch_norm': True},\n","    {'config_id': 2, 'optimizer_name': 'adam', 'output_channels': 5, 'window_size': 20, 'batch_size': 32, 'hidden_size': 64, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 80, 'use_dropout': True, 'dropout_rate': 0.3, 'use_batch_norm': True},\n","    {'config_id': 3, 'optimizer_name': 'adam', 'output_channels': 32, 'window_size': 70, 'batch_size': 32, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 150, 'use_dropout': True, 'dropout_rate': 0.1, 'use_batch_norm': True},\n","    {'config_id': 4, 'optimizer_name': 'adam', 'output_channels': 64, 'window_size': 80, 'batch_size': 32, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 150, 'use_dropout': True, 'dropout_rate': 0.1, 'use_batch_norm': True},\n","    {'config_id': 5, 'optimizer_name': 'adam', 'output_channels': 8, 'window_size': 40, 'batch_size': 128, 'hidden_size': 128, 'num_classes': 1, 'learning_rate': 0.05, 'num_epochs': 120, 'use_dropout': True, 'dropout_rate': 0.2, 'use_batch_norm': True},\n","    {'config_id': 6, 'optimizer_name': 'sgd', 'output_channels': 3, 'window_size': 10, 'batch_size': 16, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.001, 'num_epochs': 50, 'use_dropout': True, 'dropout_rate': 0.5, 'use_batch_norm': True, 'momentum': 0.9},\n","    {'config_id': 7, 'optimizer_name': 'sgd', 'output_channels': 32, 'window_size': 70, 'batch_size': 32, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 150, 'use_dropout': True, 'dropout_rate': 0.5, 'use_batch_norm': True, 'momentum': 0.3},\n","    {'config_id': 8, 'optimizer_name': 'sgd', 'output_channels': 64, 'window_size': 80, 'batch_size': 32, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 150, 'use_dropout': True, 'dropout_rate': 0.1, 'use_batch_norm': True, 'momentum': 0.2},\n","    {'config_id': 9, 'optimizer_name': 'adam', 'output_channels': 3, 'window_size': 10, 'batch_size': 16, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.001, 'num_epochs': 50, 'use_dropout': True, 'dropout_rate': 0.5, 'use_batch_norm': True},\n","    {'config_id': 10, 'optimizer_name': 'adam', 'output_channels': 16, 'window_size': 60, 'batch_size': 32, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 80, 'use_dropout': True, 'dropout_rate': 0.1, 'use_batch_norm': True},\n","    {'config_id': 11, 'optimizer_name': 'adam', 'output_channels': 6, 'window_size': 30, 'batch_size': 64, 'hidden_size': 128, 'num_classes': 1, 'learning_rate': 0.1, 'num_epochs': 120, 'use_dropout': False, 'dropout_rate': 0.0, 'use_batch_norm': True},\n","    {'config_id': 12, 'optimizer_name': 'sgd', 'output_channels': 16, 'window_size': 60, 'batch_size': 32, 'hidden_size': 32, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 80, 'use_dropout': True, 'dropout_rate': 0.1, 'use_batch_norm': True, 'momentum': 0.4},\n","    {'config_id': 13, 'optimizer_name': 'sgd', 'output_channels': 5, 'window_size': 20, 'batch_size': 32, 'hidden_size': 64, 'num_classes': 1, 'learning_rate': 0.01, 'num_epochs': 100, 'use_dropout': True, 'dropout_rate': 0.3, 'use_batch_norm': True, 'momentum': 0.8},\n","    {'config_id': 14, 'optimizer_name': 'sgd', 'output_channels': 8, 'window_size': 40, 'batch_size': 128, 'hidden_size': 128, 'num_classes': 1, 'learning_rate': 0.05, 'num_epochs': 120, 'use_dropout': True, 'dropout_rate': 0.2, 'use_batch_norm': True, 'momentum': 0.6},\n","    {'config_id': 15, 'optimizer_name': 'sgd', 'output_channels': 6, 'window_size': 30, 'batch_size': 64, 'hidden_size': 128, 'num_classes': 1, 'learning_rate': 0.1, 'num_epochs': 120, 'use_dropout': False, 'dropout_rate': 0.0, 'use_batch_norm': True, 'momentum': 0.7},\n","    {'config_id': 16, 'optimizer_name': 'sgd', 'output_channels': 10, 'window_size': 50, 'batch_size': 64, 'hidden_size': 64, 'num_classes': 1, 'learning_rate': 0.001, 'num_epochs': 100, 'use_dropout': True, 'dropout_rate': 0.5, 'use_batch_norm': True, 'momentum': 0.5}\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fhd657M1H9xB"},"outputs":[],"source":["\n","def calculate_metrics(outputs, labels):\n","    predicted = outputs.detach().round()\n","    accuracy = accuracy_score(labels.cpu().numpy(), predicted.cpu().numpy())\n","    precision = precision_score(labels.cpu().numpy(), predicted.cpu().numpy(), zero_division=0)\n","    recall = recall_score(labels.cpu().numpy(), predicted.cpu().numpy(), zero_division=0)\n","    f1 = f1_score(labels.cpu().numpy(), predicted.cpu().numpy(), zero_division=0)\n","    return accuracy, precision, recall, f1\n","\n","def train_epoch(model, data_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0\n","    total_batches = 0\n","\n","    for hrv_data, activity_data, labels in data_loader:\n","        hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(hrv_data, activity_data).squeeze(1)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * labels.size(0)\n","        accuracy, precision, recall, f1 = calculate_metrics(outputs, labels)\n","        total_accuracy += accuracy\n","        total_precision += precision\n","        total_recall += recall\n","        total_f1 += f1\n","        total_batches += 1\n","\n","    average_loss = running_loss / len(data_loader.dataset)\n","    average_accuracy = total_accuracy / total_batches\n","    average_precision = total_precision / total_batches\n","    average_recall = total_recall / total_batches\n","    average_f1 = total_f1 / total_batches\n","\n","    return average_loss, average_accuracy, average_precision, average_recall, average_f1\n","\n","def validate_epoch(model, data_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0\n","    total_batches = 0\n","\n","    with torch.no_grad():\n","        for hrv_data, activity_data, labels in data_loader:\n","            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device)\n","            outputs = model(hrv_data, activity_data).squeeze(1)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item() * labels.size(0)\n","            accuracy, precision, recall, f1 = calculate_metrics(outputs, labels)\n","            total_accuracy += accuracy\n","            total_precision += precision\n","            total_recall += recall\n","            total_f1 += f1\n","            total_batches += 1\n","\n","    average_loss = running_loss / len(data_loader.dataset)\n","    average_accuracy = total_accuracy / total_batches\n","    average_precision = total_precision / total_batches\n","    average_recall = total_recall / total_batches\n","    average_f1 = total_f1 / total_batches\n","\n","    return average_loss, average_accuracy, average_precision, average_recall, average_f1\n","\n","\n","def test_model(model, data_loader, device):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for hrv_data, activity_data, labels in data_loader:\n","            hrv_data, activity_data, labels = hrv_data.to(device), activity_data.to(device), labels.to(device)\n","            outputs = model(hrv_data, activity_data)\n","            all_outputs.append(outputs)\n","            all_labels.append(labels)\n","\n","    all_outputs = torch.cat(all_outputs).squeeze(1)\n","    all_labels = torch.cat(all_labels)\n","\n","    accuracy, precision, recall, f1 = calculate_metrics(all_outputs, all_labels)\n","\n","    return accuracy, precision, recall, f1\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQKq3TboS2sp"},"outputs":[],"source":["def run_experiment(config, all_data):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = MultimodalADHDNet(\n","        output_channels=config['output_channels'],\n","        hidden_size=config['hidden_size'],\n","        num_classes=config['num_classes'],\n","        use_batch_norm=config['use_batch_norm'],\n","        use_dropout=config['use_dropout'],\n","        dropout_rate=0.5\n","    ).to(device)\n","\n","    # Initialize the optimizer based on the configuration's optimizer name\n","    if config['optimizer_name'] == 'adam':\n","        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n","    elif config['optimizer_name'] == 'sgd':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n","\n","    criterion = nn.BCELoss()\n","\n","    train_data = preprocessData_no_windows(config, all_data['train'])\n","    val_data = preprocessData_no_windows(config, all_data['val'])\n","    test_data = preprocessData_no_windows(config, all_data['test'])\n","\n","    train_loader = DataLoader(PatientDataset(train_data), batch_size=config['batch_size'], shuffle=True)\n","    val_loader = DataLoader(PatientDataset(val_data), batch_size=config['batch_size'], shuffle=False)\n","    test_loader = DataLoader(PatientDataset(test_data), batch_size=config['batch_size'], shuffle=False)\n","\n","    final_train_metrics = {}\n","    final_val_metrics = {}\n","    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n","\n","    for epoch in range(config['num_epochs']):\n","        train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n","        val_loss, val_acc, val_prec, val_rec, val_f1 = validate_epoch(model, val_loader, criterion, device)\n","\n","        history['train_loss'].append(train_loss)\n","        history['train_acc'].append(train_acc)\n","        history['val_loss'].append(val_loss)\n","        history['val_acc'].append(val_acc)\n","\n","        print(f\"Epoch {epoch + 1}/{config['num_epochs']}: Train Loss: {round(train_loss, 4)}, Train Acc: {round(train_acc, 4)}, Val Loss: {round(val_loss, 4)}, Val Acc: {round(val_acc, 4)}\")\n","\n","        if epoch == config['num_epochs'] - 1:\n","            final_train_metrics = {\n","                'Train Loss': round(train_loss, 2),\n","                'Train Accuracy': round(train_acc, 2),\n","                'Train Precision': round(train_prec, 2),\n","                'Train Recall': round(train_rec, 2),\n","                'Train F1': round(train_f1, 2)\n","            }\n","            final_val_metrics = {\n","                'Val Loss': round(val_loss, 2),\n","                'Val Accuracy': round(val_acc, 2),\n","                'Val Precision': round(val_prec, 2),\n","                'Val Recall': round(val_rec, 2),\n","                'Val F1': round(val_f1, 2)\n","            }\n","\n","    test_accuracy, test_precision, test_recall, test_f1 = test_model(model, test_loader, device)\n","    test_metrics = {\n","        'Test Accuracy': round(test_accuracy, 2),\n","        'Test Precision': round(test_precision, 2),\n","        'Test Recall': round(test_recall, 2),\n","        'Test F1': round(test_f1, 2)\n","    }\n","\n","    return history, {**final_train_metrics, **final_val_metrics, **test_metrics}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6237121,"status":"ok","timestamp":1715007043302,"user":{"displayName":"Jivko Parapanov","userId":"13549335107747025520"},"user_tz":-120},"id":"dpLck4xUHPdj","outputId":"f8ea6264-3515-49b4-b11f-cf7733cd9b9a"},"outputs":[],"source":["results = []\n","config_histories = {}\n","for config in configurations:\n","    print(f\"Configuration: {config}\")\n","    history, test_results = run_experiment(config, all_data)\n","    config_histories[config['config_id']] = history\n","    results.append({'config': config, **test_results})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2386,"status":"ok","timestamp":1715007087298,"user":{"displayName":"Jivko Parapanov","userId":"13549335107747025520"},"user_tz":-120},"id":"JidlW6I5VzlC","outputId":"f6eb635d-f9c1-49a5-a2de-875d663c2050"},"outputs":[],"source":["def plot_all_configs(histories):\n","    for config_id, history in histories.items():\n","        plt.figure(figsize=(12, 5))\n","        plt.subplot(1, 2, 1)\n","        plt.plot(history['train_loss'], label='Train Loss')\n","        plt.plot(history['val_loss'], label='Validation Loss')\n","        plt.title(f'Config {config_id} - Loss Over Epochs')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","\n","        plt.subplot(1, 2, 2)\n","        plt.plot(history['train_acc'], label='Train Accuracy')\n","        plt.plot(history['val_acc'], label='Validation Accuracy')\n","        plt.title(f'Config {config_id} - Accuracy Over Epochs')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","        plt.legend()\n","\n","        plt.show()\n","\n","# Plotting all configurations after running all experiments\n","plot_all_configs(config_histories)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCODQhYeH4rx"},"outputs":[],"source":["# Convert results to DataFrame and save to CSV\n","results_df = pd.DataFrame(results)\n","results_df.to_csv('results_batch05_best_tunned_no_windows.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
